# DevOps RAG Pipeline

A specialized Retrieval-Augmented Generation (RAG) pipeline designed to ingest, classify, and chunk DevOps infrastructure code (Dockerfiles, GitHub Actions, Terraform) for high-quality LLM retrieval.

## ğŸš€ Features

-   **Strict File Classification**: Automatically tags files with `authority` (Critical/High), `execution_risk`, `environment`, and `service`.
-   **Smart Chunking**:
    -   **Dockerfiles**: Chunks by instruction block types (Build vs Runtime stages).
    -   **GitHub Actions**: Chunks by individual jobs locally within the workflow context.
    -   **Terraform**: Chunks by resources and modules, preserving provider context.
-   **Vector Storage**: Integrated with **ChromaDB** for local persistence and **OpenAI** for high-quality embeddings.

## ğŸ“‚ Project Structure

```
rag_pipeline/
â”œâ”€â”€ main.py                 # Entry point for ingestion & indexing
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ classifier.py       # Rule-based metadata tagging
â”‚   â”œâ”€â”€ vector_store.py     # ChromaDB wrapper with OpenAI embedding
â”‚   â””â”€â”€ chunkers/           # Specialized parsers
â”‚       â”œâ”€â”€ docker.py
â”‚       â”œâ”€â”€ github.py
â”‚       â””â”€â”€ terraform.py
â””â”€â”€ rag_pipeline_output.json # Intermediate structured output (verify before indexing)
```

## ğŸ› ï¸ Setup

1.  **Install Dependencies**:
    ```bash
    pip install -r rag_pipeline/requirements.txt
    ```

2.  **Set Environment Variables**:
    You must provide an OpenAI API Key for embedding generation.
    ```bash
    export OPENAI_API_KEY=sk-your-key-here
    ```

## ğŸƒ Usage

### 1. Run Data Ingestion
This script will:
1.  Scan `rag_data_source/`.
2.  Classify and chunk all files.
3.  Save structured chunks to `rag_pipeline_output.json`.
4.  Generate embeddings and index them into `chroma_db/`.

```bash
python3 rag_pipeline/main.py
```

### 2. Verify / Query
Run the sample query script to test retrieval:

```bash
python3 rag_pipeline/query.py "How do we build the order service?"
```

## ğŸ§  Design Principles (Phase 3)

1.  **Authority Matters**: Infrastructure as Code (Terraform) trumps Documentation (README). The classifier assigns `authority: critical` to Terraform files.
2.  **Context Preservation**: Chunks are not arbitrary text splits. They are semantic units (e.g., a "Build Job" or an "AWS EKS Cluster resource").
3.  **Risk Awareness**: Files are tagged with `execution_risk` to help the LLM decide if it's safe to execute code found in them.
